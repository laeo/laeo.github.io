<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Containers on LAEO</title><link>https://laeo.me/container/</link><description>Recent content in Containers on LAEO</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 09 Jul 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://laeo.me/container/index.xml" rel="self" type="application/rss+xml"/><item><title>三节点K3S集群部署Gitea记录</title><link>https://laeo.me/container/%E4%B8%89%E8%8A%82%E7%82%B9k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2gitea%E8%AE%B0%E5%BD%95/</link><pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate><guid>https://laeo.me/container/%E4%B8%89%E8%8A%82%E7%82%B9k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2gitea%E8%AE%B0%E5%BD%95/</guid><description>接前文所言，记录下gitea的搭建过程，处理Longhorn存储系统挂载失败的问题，并且处理老数据迁移问题。
Gitea Gitea 是一个开源社区驱动的轻量级代码托管解决方案，后端采用 Go 编写，采用 MIT 许可证.
部署 Gitea文档中并未提供K8S部署相关的说明，但是我从其github仓库中找到了官方提供的YAML部署文件，地址是 https://github.com/go-gitea/gitea/blob/master/contrib/k8s/gitea.yml ，直接下载到本地，然后根据需要进行修改即可。下面是我修改后的
gitea.yaml
--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gitea-data namespace: gitea spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: gitea namespace: gitea labels: app: gitea spec: replicas: 1 template: metadata: name: gitea labels: app: gitea spec: containers: - name: gitea image: gitea/gitea:1.12 imagePullPolicy: Always volumeMounts: - mountPath: &amp;#34;/data&amp;#34; name: &amp;#34;data&amp;#34; ports: - containerPort: 22 name: ssh protocol: TCP - containerPort: 3000 name: http protocol: TCP restartPolicy: Always securityContext: fsGroup: 1000 volumes: - name: &amp;#34;data&amp;#34; persistentVolumeClaim: claimName: gitea-data selector: matchLabels: app: gitea --- apiVersion: v1 kind: Service metadata: name: gitea-web namespace: gitea labels: app: gitea-web spec: ports: - port: 80 targetPort: 3000 name: http selector: app: gitea --- apiVersion: v1 kind: Service metadata: name: gitea-ssh namespace: gitea labels: app: gitea-ssh spec: ports: - port: 22 targetPort: 22 nodePort: 30022 name: ssh selector: app: gitea type: NodePort --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: gitea namespace: gitea annotations: cert-manager.</description></item><item><title>k8s中drone-kube-runner容器无网络问题</title><link>https://laeo.me/container/k8s%E4%B8%ADdrone-kube-runner%E5%AE%B9%E5%99%A8%E6%97%A0%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98/</link><pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate><guid>https://laeo.me/container/k8s%E4%B8%ADdrone-kube-runner%E5%AE%B9%E5%99%A8%E6%97%A0%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98/</guid><description>起因 用腾讯云CVM组的K8S集群跑drone的drone-kube-runner，执行项目的容器镜像构建逻辑，但是出现构建流程卡住的问题。 多重试几次，偶尔还能正常构建起来。但是今天重试了好几次，一直卡住。卡住的地方还都一样， apk update 这里。 因为基础镜像是用的 alpine，就将其自带的apk的更新地址换为了华中科大的镜像站，并没有多少改善，还是卡住。 然后又换到阿里云的镜像站，最后换到腾讯云的镜像站，一直都这样，很奇怪。 实在没招，直接 kubectl exec 进容器里看看到底啥情况。 进去之后下意识执行了 apk update ，发现报了几个网络错误。然后又看了下容器的IP地址、网卡配置，都没啥问题。 尝试 ping 了下百度，无用。又看了 NS 服务器的配置，也正常，但是执行 nslookup 报错。 彻底没招，谷歌走起……
搜集资料 谷歌搜了下 k8s drone no network ，第一条记录就引起了我的注意：Drone in Kubernetes has network issue。 查看了下这个帖子，其中一条说他自己的解决方案的，我感觉有可能也是这个问题，遂跟着对方给的连接看了过去。 然后又在自己的容器内外查看、对比了下情况，发现我的网卡 MTU 设置跟他的一样，问题定位到了。 当然为了确保判断正确，我还是手动执行了下 ifconfig docker0 mtu 1440 up 来测试网络是否正常，当然结果确实是正常了。
解决它 找到问题，人家回复中又给出了修正的方法，那不必多说，直接拷贝其给出的环境变量配置到项目的 .drone.yaml 文件中，提交完事儿！
environment: PLUGIN_MTU: 1440 为了方便我是直接设置的 Pipeline 等级的环境变量。</description></item><item><title>腾讯云CVM搭建最新K8S集群过程记录</title><link>https://laeo.me/container/%E8%85%BE%E8%AE%AF%E4%BA%91cvm%E6%90%AD%E5%BB%BA%E6%9C%80%E6%96%B0k8s%E9%9B%86%E7%BE%A4%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>https://laeo.me/container/%E8%85%BE%E8%AE%AF%E4%BA%91cvm%E6%90%AD%E5%BB%BA%E6%9C%80%E6%96%B0k8s%E9%9B%86%E7%BE%A4%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/</guid><description>说明 穷人专用的在腾讯云不同账号下的CVM上搭建K8S集群。
原料 官方指南一份 腾讯云CVM一份（配置2C4G5M）（运行CentOS 7.4） 微软CN提供的gcr.io镜像一份 阿里云提供的k8s镜像一份 烹饪 根据 容器运行时 文档介绍选择中意的容器运行时程序，跟以往不同，我没有选择 docker，而是选择了 containerd，差别请前往 如何选择 Containerd 和 Docker 这篇文档。 腾讯云连 Docker 的源太慢了，还是用阿里提供的镜像吧。
需要注意的是，当我们根据文档中的说明生成了默认的 containerd 配置文件后，我们需要手动将其中的 plugins.cri.sandbox_image 的地址改为微软CN提供的 镜像 的地址，否则下载不到沙盒镜像，集群将无法正常启动。
还需要注意的是，containerd 的 CLI 工具是 crictl ，常用命令与 docker 一致，具体需要查询官网。安装好 containerd 后是无法直接使用 crictl 查看容器信息的，会报错连接超时。原因是 crictl 默认的后端地址是 unix:///var/run/dockershim.sock ，而 containerd 使用的地址是 unix:///run/containerd/containerd.sock ，可以设置环境变量 CONTAINER_RUNTIME_ENDPOINT 来便于使用。
编辑 containerd 的配置文件，使用 docker registry 的镜像地址 https://dockerhub.azk8s.cn 来加速镜像拉取。
根据 官方文档 安装 kubeadm，安装过程中使用阿里云k8s镜像来加速安装 kubeadm、kubelet、kubectl 等软件。 安装前需要根据文档做系统初始化，打开流量转发等功能。记得一定要执行 modprobe br_netfilter ，否则预检会报错 /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1。</description></item><item><title>kubernetes 集群安装分布式存储组件 rook-ceph</title><link>https://laeo.me/container/kubernetes%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%BB%84%E4%BB%B6rook-ceph/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>https://laeo.me/container/kubernetes%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%BB%84%E4%BB%B6rook-ceph/</guid><description>技术选型 根据官方文档说明，k8s 提供了不同存储系统，可以直接根据需求选择不同的存储系统进行部署。由于分布式系统的特点，优先选择同样支持分布式部署的存储系统。为了方便部署，我采用 rook 来部署和管理存储集群。
什么是 rook？ Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.
来自官方文档的说明，云原生的存储适配器。根据实际使用体验，rook 是用来对开源云原生存储系统的部署、管理进行管理的工具。通过 rook 可以快捷的部署一套私有的存储集群系统，从官网文档也可以看出其适配了多个云存储集群系统。
部署 rook-ceph 集群 参照 快速上手指导 所列各个存储引擎的版本状态，选择采用已发布正式版本的 ceph 存储系统。
Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared file systems with years of production deployments.
为了快速可控的部署 rook 服务，采用开源的 kubernetes 包管理器 helm 来安装，如果希望获得更多自定义能力，可以参照 官方部署文档 部署 rook-operator，有关 helm 的安装自行查询官方文档。</description></item><item><title>为 k8s 集群安装 traefik 作为 Ingress 提供商</title><link>https://laeo.me/container/%E4%B8%BAk8s%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85traefik%E4%BD%9C%E4%B8%BAingress%E6%8F%90%E4%BE%9B%E5%95%86/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>https://laeo.me/container/%E4%B8%BAk8s%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85traefik%E4%BD%9C%E4%B8%BAingress%E6%8F%90%E4%BE%9B%E5%95%86/</guid><description>在上一篇文章中，我提到了使用 Ingress 来暴露集群内部服务到公网，那如何为私有的 K8S 集群配置一个 Ingress 服务呢？
什么是 Ingress Ingress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the ingress resource.
根据文档说明，Ingress 是一种通过规则来对外网到集群内的 HTTP/HTTPS 流量进行路由控制的资源。
官方推荐了数个 Ingress 控制器，具体可以查看 这里。由于之前有过 Traefik 相关使用经验，所以  在挑选控制器的时候我依然选择了 Traefik。
安装 Traefik 参照  文档的说明可以手动安装，获得更强的自定义能力。因为方便和可控，我依然选择使用 Helm 来安装，也是在官方文档中说明的流程。
helm install -n traefik --namespace kube-system --values values.yaml stable/traefik 其中 values.yaml 文件存放了该包的配置项，可以通过 helm inspect stable/traefik 查看该包的简介，在其中可以查看到所有的配置选项。</description></item><item><title>helm 安装 chart 报 no route to host 错误</title><link>https://laeo.me/container/helm%E5%AE%89%E8%A3%85chart%E6%8A%A5no-route-to-host%E9%94%99%E8%AF%AF/</link><pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate><guid>https://laeo.me/container/helm%E5%AE%89%E8%A3%85chart%E6%8A%A5no-route-to-host%E9%94%99%E8%AF%AF/</guid><description>错误信息 $ k8s helm install stable/nginx-ingress --name nginx --set rbac.create=true --namespace kube-system Error: forwarding ports: error upgrading connection: error dialing backend: dial tcp *.*.*.*:10250: connect: no route to host 解决方案 清理对应节点的 iptables 规则即可，
systemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start kubelet systemctl start docker 重新一试果然正常了，但是直勾勾重启节点上的 kubelet 和 docker，又导致 rook 安装的 ceph 存储集群炸了，相关的 pod 处于 pending 状态。</description></item><item><title>Kubeadm 创建 Kubernetes 集群备忘录</title><link>https://laeo.me/container/kubeadm%E5%88%9B%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4%E5%A4%87%E5%BF%98%E5%BD%95/</link><pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate><guid>https://laeo.me/container/kubeadm%E5%88%9B%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4%E5%A4%87%E5%BF%98%E5%BD%95/</guid><description>许久不曾写点东西，这次迫于安装 Kubeadm 过程有点麻烦，想了想还是写下来，以后使用的时候也省的再费脑细胞。
配置要求 基础的服务器硬件要求参照 官方文档。
云服务器提供商？ 慎重选择阿里云之类的有公共网关的云服务，因为这些提供商所提供的公网 IP 是没有绑定到虚拟服务器上的，而是通过路由设施映射过去。Kubeadm 在创建集群的时候，会让 etcd 监听在其获取的本地网卡 IP 上，如果采用阿里云这样的提供商，它获取的网卡 IP 只会是内网 IP。结果就是，其它外网的节点无法正常链接上主节点的 etcd 服务，所以集群无法正常使用。
Swap 禁止使用？ 根据官方文档的说明，Kubernetes 集群需要的是稳定性，而依托于硬盘空间而生的 swap 空间，在读写性能上无法比肩常规内存空间，稳定性无法得到保证，所以禁止使用它。
有的服务器默认就启用了 swap 空间，我们只需编辑 /etc/fstab 文件，注释掉其中 swap 类型的挂载操作，并重启服务器即可。
内核参数调整？ 参照 Kubeadm 安装文档执行以下命令，
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sysctl --system /proc/sys/net/bridge/bridge-nf-call-iptables not found 参照 issue，执行以下指令
modprobe br_netfilter 即可解决。
使用 Docker-CE 作为运行时 Kubernetes 每个版本都有其兼容的 docker 版本，为了新特性一般都安装最新版 docker-ce，除了参照 官方文档，最简单的安装方式</description></item><item><title>docker 实现新建容器后自动创建 nginx 反向代理</title><link>https://laeo.me/container/docker%E5%AE%9E%E7%8E%B0%E6%96%B0%E5%BB%BA%E5%AE%B9%E5%99%A8%E5%90%8E%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BAnginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</link><pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate><guid>https://laeo.me/container/docker%E5%AE%9E%E7%8E%B0%E6%96%B0%E5%BB%BA%E5%AE%B9%E5%99%A8%E5%90%8E%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BAnginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</guid><description>nginx-proxy 介绍 nginx-proxy 是一款开源的、根据容器自动创建 nginx 反向代理的软件，基于 docker-gen 开发。使用该软件，我们可以实现在创建 web 项目时，快速构建线上测试环境，免去手动配置 nginx 的痛苦。另外，搭配另一款软件——letsencrypt-nginx-proxy-companion，更能实现自动申请 let&amp;rsquo;s encrypt 免费证书，轻松搭建 HTTPS 站点，可以方便的用于某些线上环境。
动手使用 该软件使用非常简单，全程基于 docker 容器软件，只需两步即可。此处我放置的示例命令来自 letsencrypt-nginx-proxy-companion 的使用说明，推荐构建 HTTPS 站点。
mkdir /var/certs # 创建证书存放目录 docker run -d -p 80:80 -p 443:443 \ --name nginx-proxy \ -v /var/certs:/etc/nginx/certs:ro \ -v /etc/nginx/vhost.d \ -v /usr/share/nginx/html \ -v /var/run/docker.sock:/tmp/docker.sock:ro \ --label com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy \ --restart=always \ jwilder/nginx-proxy docker run -d \ -v /var/certs:/etc/nginx/certs:rw \ -v /var/run/docker.sock:/var/run/docker.sock:ro \ --volumes-from nginx-proxy \ --restart=always \ jrcs/letsencrypt-nginx-proxy-companion 上述命令执行完成后，我们就正式运行起了 docker-proxy 与 letsencrypt-nginx-proxy-companion 服务。</description></item></channel></rss>